{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28abf9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vncorenlp\n",
    "# !pip install angdetect\n",
    "# !pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a70f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# This funtion will format \"publishedAtDate\" column for later usage\n",
    "def formatToDatetime(date_string):\n",
    "    date_string_without_fraction = date_string[:-5] + 'Z'\n",
    "    date_format = \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "    return datetime.strptime(date_string_without_fraction, date_format)\n",
    "\n",
    "# How to apply to dataframe\n",
    "# df['date_column'] = df['date_column'].apply(lambda x: convert_to_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "087fb8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "def removeOtherLanguage(data):\n",
    "    for i in range(len(data)):\n",
    "        try: \n",
    "            if detect(str(data.loc[i, 'text'])) != \"vi\":\n",
    "                #print(str(data.loc[i, 'text']), i)\n",
    "                data = data.drop(i)\n",
    "        except:\n",
    "            #print(str(data.loc[i, 'text']))\n",
    "            data = data.drop(i)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125a59cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeReplace(text):\n",
    "    replacements = {\n",
    "        \"òa\": \"oà\", \"óa\": \"oá\", \"ỏa\": \"oả\", \"õa\": \"oã\", \"ọa\": \"oạ\",\n",
    "        \"òe\": \"oè\", \"óe\": \"oé\", \"ỏe\": \"oẻ\", \"õe\": \"oẽ\", \"ọe\": \"oẹ\",\n",
    "        \"ùy\": \"uỳ\", \"úy\": \"uý\", \"ủy\": \"uỷ\", \"ũy\": \"uỹ\", \"ụy\": \"uỵ\",\n",
    "        \"Ủy\": \"Uỷ\", \"\\n\": \".\" , \"\\t\": \".\"  # Add more replacements as needed\n",
    "    }\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "def unicode(data):\n",
    "    data['text'] = data['text'].apply(unicodeReplace)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "957de57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f92140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base Vietnamese alphabet without tone marks\n",
    "vietnamese_alphabet = \"aăâbcdđeêghiklmnoôơpqrstuưvwxy\"\n",
    "vietnamese_letter_with_tone = \"áàạãảắằẵẳặấầẩẫậéèẻẽẹềếểễệòóỏõọồốổỗộờớởỡợúùũủụứừửữựíìĩỉịýỳỹỷỵ\"\n",
    "\n",
    "# Create uppercase Vietnamese letters with tone marks\n",
    "uppercase_vietnamese_letters_with_tone = [char.upper() for char in vietnamese_letter_with_tone]\n",
    "uppercase_vietnamese_alphabet = vietnamese_alphabet.upper()\n",
    "\n",
    "# Combine the lists into strings\n",
    "lowercase_string = vietnamese_alphabet + \"\".join(vietnamese_letter_with_tone)\n",
    "uppercase_string = uppercase_vietnamese_alphabet + \"\".join(uppercase_vietnamese_letters_with_tone)\n",
    "allcase_string = lowercase_string + uppercase_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc329a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation = \"!\\\"#$%&'()*+,./:;<=>?@[\\]^_`{|}~\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e0bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stickyPreprocess(data):\n",
    "    def processText(text):\n",
    "        result = []\n",
    "        for letter_id in range(len(text) - 2):\n",
    "            prev, letter, after = text[letter_id], text[letter_id + 1], text[letter_id + 2]\n",
    "\n",
    "            if letter in punctuation:\n",
    "                if prev in allcase_string:\n",
    "                    result.append(letter_id + 1)\n",
    "                if after in allcase_string:\n",
    "                    result.append(letter_id + 2)\n",
    "\n",
    "            if letter in uppercase_string and prev in lowercase_string and letter_id != 0:\n",
    "                result.extend([letter_id, letter_id + 1])\n",
    "\n",
    "        for index in reversed(result):\n",
    "            text = text[:index] + \" \" + text[index:]\n",
    "\n",
    "        return text\n",
    "\n",
    "    data['text'] = data['text'].apply(processText)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "244ea5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "annotator = VnCoreNLP(\"../vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61dafbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "abbreviation_dict = '../vncorenlp/abbreviation_dictionary_vn.xlsx'\n",
    "df = pd.read_csv('../vncorenlp/abbreviation_dictionary_vn.csv')\n",
    "abbreviation_dict = df.set_index(\"abbreviation\")[\"meaning\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f5604c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviationPreprocess(data):\n",
    "    def replaceWord(word, dictionary):\n",
    "        return dictionary.get(word, word)\n",
    "\n",
    "    def processText(text):\n",
    "        annotator_text = annotator.tokenize(text)\n",
    "\n",
    "        tokens = [it for sublist in annotator_text for it in sublist if it != '_']\n",
    "        tokens = [replaceWord(it.lower(), abbreviation_dict) for it in tokens]\n",
    "\n",
    "        sentences = [' '.join(sublist) for sublist in annotator_text]\n",
    "\n",
    "        return pd.Series([' '.join(tokens), sentences], index=['text', 'sentences'])\n",
    "\n",
    "    data[['text', 'sentences']] = data['text'].apply(processText)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0de530d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import underthesea\n",
    "\n",
    "def sentimentCal(sentences):\n",
    "    sentiments = [underthesea.sentiment(text) for text in sentences]\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f917ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model = joblib.load('../data_result/model.joblib')\n",
    "selected_columns = [\"pos_prop\", \"neg_prop\", \"reviewLength\", \"reviewHour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93d9c3e7-f77e-466c-bab5-59a7d96c02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pymongo import MongoClient\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b79ec55-9e93-4381-a61d-7fa6f589c913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.241.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PREDICT_RATINGS_OF_GOOGLE_LOCAL_REVIEWS_IE212_O11_GROUP7</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e5d3b87280>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scala_version = '2.12'  # your scala version\n",
    "spark_version = '3.5.0' # your spark version\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.6.0' #your kafka version\n",
    "]\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"PREDICT_RATINGS_OF_GOOGLE_LOCAL_REVIEWS_IE212_O11_GROUP7\").config(\"spark.jars.packages\", \",\".join(packages)).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "765f0837-57f3-44c6-8aa8-0bf78acdca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of KafkaConsumer class\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "topic_name = 'PREDICT_RATINGS_OF_GOOGLE_LOCAL_REVIEWS_IE212_O11_GROUP7'\n",
    "\n",
    "kafkaDf = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", bootstrap_servers)\\\n",
    "                .option(\"subscribe\", topic_name)\\\n",
    "                .option(\"startingOffsets\", \"earliest\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edda02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to local server\n",
    "client = MongoClient(\"mongodb://127.0.0.1:27017/\")\n",
    "\n",
    "# Create database called animals\n",
    "mydb = client[\"ie212_o11_group7\"]\n",
    "\n",
    "# Create Collection (table) called shelterA\n",
    "collection = mydb.reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55cd9977-4708-4da9-ac7c-b87122e8908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reviewId': 'ChZDSUhNMG9nS0VJQ0FnSUR0bklXX1R3EAE', 'placeId': 'ChIJPZM8uVYpdTERLvi8c7WWMtU', 'title': '1 Dessert & Cafe', 'location/lat': 10.79644, 'location/lng': 106.6857013, 'categories': 'Quán cà phê', 'categoryName': 'Quán cà phê', 'reviewerId': '1.03E+20', 'name': 'Oanh Huỳnh', 'stars': None, 'text': 'quán nên đổi ghế sang ghế có tựa , ghế tròn nhỉ như này ngồi khá mỏi lưng . đồ uống cũng cần cải_thiện . mình thử bing su , trà_sữa , đá xay đều bị nhạt , không ngon . quán trang_trí màu đẹp .', 'publishedAtDate': 1706930464000, 'last_update_time': 1707376249.8936727, 'sentences': ['Quán nên đổi ghế sang ghế có tựa , ghế tròn nhỉ như này ngồi khá mỏi lưng .', 'Đồ uống cũng cần cải_thiện .', 'Mình thử bing su , trà_sữa , đá xay đều bị nhạt , không ngon .', 'Quán trang_trí màu đẹp'], 'sentiment': ['neutral', 'neutral', 'positive', 'positive'], 'reviewHour': 3, 'reviewLength': 192, 'num_sentiments': 4, 'pos_prop': 0.5, 'neg_prop': 0.0, 'Predict_rating': 2.3424880711}\n",
      "{'reviewId': 'ChZDSUhNMG9nS0VJQ0FnSUR0bklXX1R3EAE', 'placeId': 'ChIJPZM8uVYpdTERLvi8c7WWMtU', 'title': '1 Dessert & Cafe', 'location/lat': 10.79644, 'location/lng': 106.6857013, 'categories': 'Quán cà phê', 'categoryName': 'Quán cà phê', 'reviewerId': '1.03E+20', 'name': 'Oanh Huỳnh', 'stars': None, 'text': 'quán nên đổi ghế sang ghế có tựa , ghế tròn nhỉ như này ngồi khá mỏi lưng . đồ uống cũng cần cải_thiện . mình thử bing su , trà_sữa , đá xay đều bị nhạt , không ngon . quán trang_trí màu đẹp .', 'publishedAtDate': 1706930464000, 'last_update_time': 1707376249.8936727, 'sentences': ['Quán nên đổi ghế sang ghế có tựa , ghế tròn nhỉ như này ngồi khá mỏi lưng .', 'Đồ uống cũng cần cải_thiện .', 'Mình thử bing su , trà_sữa , đá xay đều bị nhạt , không ngon .', 'Quán trang_trí màu đẹp'], 'sentiment': ['neutral', 'neutral', 'positive', 'positive'], 'reviewHour': 3, 'reviewLength': 192, 'num_sentiments': 4, 'pos_prop': 0.5, 'neg_prop': 0.0, 'Predict_rating': 2.3424880711}\n",
      "{'reviewId': 'ChdDSUhNMG9nS0VJQ0FnSUR0bU5hMmtRRRAB', 'placeId': 'ChIJPZM8uVYpdTERLvi8c7WWMtU', 'title': '1 Dessert & Cafe', 'location/lat': 10.79644, 'location/lng': 106.6857013, 'categories': 'Quán cà phê', 'categoryName': 'Quán cà phê', 'reviewerId': '1.02E+20', 'name': 'Trọng Việt Đinh', 'stars': None, 'text': '1 dessert & cafe 277 hoàng_sa , tân_định . chỉ phù_hợp cho làm_việc trực tuyến . có bán bánh . giá trung_bình + . hơi ồn . .', 'publishedAtDate': 1706866730000, 'last_update_time': 1707376261.8991253, 'sentences': ['1 dessert & cafe 277 Hoàng_sa , tân_định .', 'Chỉ phù_hợp cho làm_việc online .', 'Có bán bánh .', 'Giá trung_bình + .', 'Hơi ồn .'], 'sentiment': ['positive', 'neutral', 'neutral', 'negative', 'negative'], 'reviewHour': 9, 'reviewLength': 124, 'num_sentiments': 5, 'pos_prop': 0.2, 'neg_prop': 0.4, 'Predict_rating': 3.0353781552}\n",
      "{'reviewId': 'ChdDSUhNMG9nS0VJQ0FnSUR0NmRMNndBRRAB', 'placeId': 'ChIJC8n56g0pdTER0YYOkKUfacU', 'title': '27 ELITE SHOPPING COMPLEX', 'location/lat': 10.8120578, 'location/lng': 106.7000769, 'categories': 'Cửa hàng quần áo', 'categoryName': 'Cửa hàng quần áo', 'reviewerId': '1.18E+20', 'name': 'trung võ', 'stars': None, 'text': 'áo đẹp nhân_viên nhiệt_tình .', 'publishedAtDate': 1707136918000, 'last_update_time': 1707376273.933766, 'sentences': ['áo đẹp nhân_viên nhiệt_tình'], 'sentiment': ['positive'], 'reviewHour': 12, 'reviewLength': 29, 'num_sentiments': 1, 'pos_prop': 1.0, 'neg_prop': 0.0, 'Predict_rating': 5.2023484591}\n",
      "break\n",
      "Live view ended...\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from IPython.display import display, clear_output\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    "\n",
    "# Khởi tạo SparkSession\n",
    "spark = SparkSession.builder.appName(\"PREDICT_RATINGS_OF_GOOGLE_LOCAL_REVIEWS_IE212_O11_GROUP7\").getOrCreate()\n",
    "\n",
    "# Định nghĩa schema cho dữ liệu JSON\n",
    "json_schema = StructType([\n",
    "            StructField(\"reviewId\", StringType(), True),\n",
    "            StructField(\"placeId\", StringType(), True),\n",
    "            StructField(\"title\", StringType(), True),\n",
    "            StructField(\"location/lat\", DoubleType(), True),\n",
    "            StructField(\"location/lng\", DoubleType(), True),\n",
    "            StructField(\"categories\", StringType(), True),\n",
    "            StructField(\"categoryName\", StringType(), True),\n",
    "            StructField(\"reviewerId\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"stars\", IntegerType(), True),\n",
    "            StructField(\"text\", StringType(), True),\n",
    "            StructField(\"publishedAtDate\", StringType(), True),\n",
    "            StructField(\"last_update_time\", DoubleType(), True)\n",
    "            ])\n",
    "\n",
    "# Đọc dữ liệu từ Kafka topic và chuyển đổi thành DataFrame\n",
    "kafka_stream_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", bootstrap_servers) \\\n",
    "    .option(\"subscribe\", topic_name) \\\n",
    "    .load()\n",
    "\n",
    "kafkaDf = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", bootstrap_servers)\\\n",
    "                .option(\"subscribe\", topic_name)\\\n",
    "                .option(\"startingOffsets\", \"earliest\").load()\n",
    "\n",
    "# Chuyển đổi giá trị từ JSON string sang struct với schema đã định nghĩa\n",
    "json_stream_df = kafkaDf.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(\"value\", json_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Hàm callback để cập nhật average_rating\n",
    "def update_average_rating(row):\n",
    "    # Kết nối tới MongoDB\n",
    "    client = MongoClient(\"mongodb://127.0.0.1:27017/\")\n",
    "    mydb = client[\"ie212_o11_group7\"]\n",
    "    places_collection = mydb.places\n",
    "    \n",
    "    # Lấy thông tin từ row\n",
    "    place_id = row['placeId']\n",
    "    stars = row['stars'] or 0\n",
    "    predict_rating = row['Predict_rating']\n",
    "    \n",
    "    # Lấy thông tin từ MongoDB\n",
    "    place_info = places_collection.find_one({'placeId': place_id})\n",
    "    \n",
    "    if place_info:\n",
    "        total_rows = place_info['total_rows']\n",
    "\n",
    "        current_average_rating = place_info['average_rating']\n",
    "        new_average_rating = (current_average_rating * total_rows + stars) / (total_rows + 1)\n",
    "\n",
    "        # Tính toán average_rating_new\n",
    "        current_average_predict_rating = place_info['average_predict_rating']\n",
    "        new_average_predict_rating = (current_average_predict_rating * total_rows + predict_rating) / (total_rows + 1)\n",
    "        \n",
    "        # Cập nhật dữ liệu trong MongoDB\n",
    "        places_collection.update_one({'placeId': place_id},\n",
    "                                     {'$set': {'average_rating': new_average_rating}})\n",
    "\n",
    "        # Cập nhật dữ liệu trong MongoDB\n",
    "        places_collection.update_one({'placeId': place_id},\n",
    "                                     {'$set': {'average_predict_rating': new_average_predict_rating,\n",
    "                                               'total_rows': total_rows + 1}})\n",
    "\n",
    "x = 0\n",
    "while True:\n",
    "    try:\n",
    "        # Sắp xếp DataFrame theo thời gian cập nhật giảm dần và chỉ lấy dòng đầu tiên\n",
    "        newest_review_df = json_stream_df.sort(col(\"last_update_time\").desc()).limit(1)\n",
    "        # newest_review_df = json_stream_df.limit(1)\n",
    "        newest_review_df_pandas = newest_review_df.toPandas()\n",
    "\n",
    "        # Check if the pandas DataFrame is empty\n",
    "        if newest_review_df_pandas.empty:\n",
    "            continue\n",
    "        newest_review_df_pandas = removeOtherLanguage(newest_review_df_pandas)\n",
    "        if newest_review_df_pandas.empty:\n",
    "            continue\n",
    "        \n",
    "        newest_review_df_pandas = unicode(newest_review_df_pandas)\n",
    "        newest_review_df_pandas['text'] = newest_review_df_pandas['text'].apply(remove_emojis)\n",
    "        newest_review_df_pandas = stickyPreprocess(newest_review_df_pandas)\n",
    "        newest_review_df_pandas = abbreviationPreprocess(newest_review_df_pandas)\n",
    "        newest_review_df_pandas[\"sentiment\"] = newest_review_df_pandas[\"sentences\"].apply(sentimentCal)\n",
    "\n",
    "        newest_review_df_pandas['sentiment'] = newest_review_df_pandas['sentiment'].apply(lambda sentiments: [\"neutral\" if sentiment is None else sentiment for sentiment in sentiments])\n",
    "\n",
    "        newest_review_df_pandas[\"text\"] = [item + \" .\" for item in newest_review_df_pandas[\"text\"]]\n",
    "        newest_review_df_pandas['publishedAtDate'] = newest_review_df_pandas['publishedAtDate'].apply(lambda x: formatToDatetime(x))\n",
    "        newest_review_df_pandas[\"reviewHour\"] = [item.hour for item in newest_review_df_pandas[\"publishedAtDate\"]]\n",
    "        newest_review_df_pandas[\"reviewLength\"] = [len(item) for item in newest_review_df_pandas[\"text\"]]\n",
    "\n",
    "        count_pos = newest_review_df_pandas['sentiment'].apply(lambda sentiments: sum(sentiment == \"positive\" for sentiment in sentiments))\n",
    "        count_neg = newest_review_df_pandas['sentiment'].apply(lambda sentiments: sum(sentiment == \"negative\" for sentiment in sentiments))\n",
    "\n",
    "        newest_review_df_pandas['num_sentiments'] = newest_review_df_pandas['sentiment'].apply(lambda sentiments: len(sentiments) if sentiments else 0)\n",
    "\n",
    "        newest_review_df_pandas['pos_prop'] = count_pos / newest_review_df_pandas['num_sentiments']\n",
    "        newest_review_df_pandas['neg_prop'] = count_neg / newest_review_df_pandas['num_sentiments']\n",
    "\n",
    "        X = newest_review_df_pandas[selected_columns].copy()\n",
    "        newest_review_df_pandas[\"Predict_rating\"] = model.predict(X)\n",
    "\n",
    "        # newest_review_df = newest_review_df.drop(columns = ['text'], ['pos_prop'], ['neg_prop'], [])\n",
    "        \n",
    "        # Chuyển DataFrame thành JSON string và lấy giá trị cột 'value'\n",
    "        # message_value = newest_review_df.toJSON().first()\n",
    "        json_string = newest_review_df_pandas.to_json(orient='records')\n",
    "        json_list = json.loads(json_string)\n",
    "        first_row = json_list[0]\n",
    "\n",
    "        # In ra giá trị để kiểm tra\n",
    "        #print(message_value)\n",
    "        print(first_row)\n",
    "\n",
    "        # Insert documents (rows) into the database's collection (table)\n",
    "        collection.insert_one(first_row)\n",
    "        \n",
    "        # Gọi hàm callback để cập nhật average_rating\n",
    "        update_average_rating(first_row)\n",
    "        sleep(10)\n",
    "        # clear_output(wait=True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"break\")\n",
    "        break\n",
    "\n",
    "print(\"Live view ended...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494371ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: The number of columns doesn't match.\nOld column names (13): reviewId, placeId, title, location/lat, location/lng, categories, categoryName, reviewerId, name, stars, text, publishedAtDate, last_update_time\nNew column names (0): ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnewest_review_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Programs File\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:5486\u001b[0m, in \u001b[0;36mDataFrame.toDF\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   5481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   5482\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5483\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_LIST_OF_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5484\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcols\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   5485\u001b[0m         )\n\u001b[1;32m-> 5486\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mE:\\Programs File\\spark-3.5.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mE:\\Programs File\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: The number of columns doesn't match.\nOld column names (13): reviewId, placeId, title, location/lat, location/lng, categories, categoryName, reviewerId, name, stars, text, publishedAtDate, last_update_time\nNew column names (0): "
     ]
    }
   ],
   "source": [
    "newest_review_df.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669326ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
